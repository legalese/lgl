#+TITLE: LGL SDK TESTS

* Testing the lgl executable

The lgl executable is the preferred way to exercise back-end functionality. It's what users use, so we use it too.

This test suite runs the lgl executable against a range of environment
variables, command-line parameters, and STDIN inputs. For each input,
we expect certain output. Now you're thinking functionally!

First we set up a master directory to contain the output of one or more test runs.

A test run operates against a specific endpoint, using a specific protocol version, operating one or more service families.

Together, multiple runs will provide complete test coverage.

** How tests pass or fail

The results of each test are saved into ~*.out~ (for STDOUT) and ~*.err~ for (STDERR) files.

Then directory's run output is diffed against a known reference set of expectations.

If reference is equivalent to the output, the test passes.

** Setting up the Test-Run Master Directory

A test run starts at a particular point in time. We create a directory accordingly.

#+NAME: lgltest-setup-common
#+BEGIN_SRC sh :eval no :noweb yes :noweb-sep ""
# do not edit *.sh files! they are automatically tangled out of tests/README.org with C-c C-v t

testdir=testruns/`date +%Y%m%d-%H%M%S`
echo "*** saving to $testdir"
mkdir -p $testdir
cd $testdir
export LGL_URI
function test_services { (
  mkdir $subdir; cd $subdir
  echo well, here we are working against $LGL_URI
  <<arguments>>
) }
#+END_SRC

Within that master directory, we may create one or more subdirectories to represent specific combinations of endpoint and version.

Within those subdirectories, we create one or more files; each file represents a particular test point, a particular ~lgl~ invocation.

** Dimensions of Test Coverage

To achieve full coverage we need to take every point in test space across the following dimensions:

*** liveness: development / production
:PROPERTIES:
:header-args: :noweb-ref liveness
:END:

**** development
#+NAME: liveness_development
| local | http://localhost/api/ |

**** production
#+NAME: liveness_production
| v2  | https://legalese.com/api/corpsec                     |
| 1a  | https://ap-southeast-1a-api.legalese.com/api/corpsec |
| 1b  | https://ap-southeast-1b-api.legalese.com/api/corpsec |
| api | https://api.legalese.com/api/corpsec                 |

*** version: 0.9 / 1.0 / 1.1

#+NAME: version
| v0.9 | --version=v0.9 |
| v1.0 | --version=v0.9 |
| v1.1 |                |

*** command line arguments
:PROPERTIES:
:header-args: :noweb-ref arguments
:END:

lgl takes arguments. We invoke it differently and see if each run matches expectations.

Each test run is operated out of a shell script. The essence of each shell script looks like this:

**** basics: help / account setup

Before we get into the meat of the testing we see if the basics are in place.

***** help

First we make sure ~lgl~ is properly installed, and responding to cries for ~help~.

#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="help")>>
#+END_SRC

#+RESULTS:
#+begin_example
usage: lgl [help] command subcommand ...
commands:
    help [command]          view more details about command
    init                    initialize account
    demo                    walks you through key functionality
    bizfile / corpsec       retrieves company details from government backends
    proforma                creates paperwork from templates, filled with JSON parameters

    login                   reinitialize account if lglconfig.json has gone missing
    config                  manipulate lglconfig.json. Or you could just edit it yourself.

options:
    --test                  all commands will run in test mode against the dev sandbox
    --verbose               verbose logging
    --world=some.json       load environment context from some.json file
    --config=conf.json      load configuration from conf.json instead of default ./lglconfig.json

environment variables:
    LGL_VERBOSE   set to truthy to get more verbosity


try: lgl help demo

#+end_example

***** -t init

We set up some test credentials. Without them, the rest of the tests wouldn't work.

#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="init -t")>>
#+END_SRC

This should succeed with:

#+RESULTS:
: You have set up a Legalese account with test credentials.
: Those credentials have been saved to lglconfig.json
: Commands will work with limited functionality for demo purposes.
: When you are ready to use the system for real,
:   rm lglconfig.json
:   lgl init <email>
: 

If it fails, we get

#+RESULTS:
: lgl: init: config file lglconfig.json already exists; refusing to init.
:     If you are sure you want to re-initialize,
:     and you are prepared to create a new account with a different email address,
:     delete lglconfig.json and run init again with the new email address.
:     Or just go to a different directory, without a lglconfig.json file, and lgl init.


**** service families: proforma / bizfile / workflow

Then we get into the test of each service.

***** proforma schemalist

#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="proforma schemalist")>>
#+END_SRC

#+RESULTS: schemalist
: {
:   "example1a": "Example Template One Alpha",
:   "example1b": "Example Template One Beta",
:   "example1c": "Example Template One C",
:   "example1d": "Example Template One D",
:   "example1e": "Example Template One E",
:   "example1f": "Example Template One F",
:   "hw3": "Hello World 3"
: }

***** proforma schemalist hw3

#+NAME: capture schemalist hw3
#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="proforma schemalist hw3")>>
#+END_SRC

#+RESULTS: capture schemalist hw3

***** proforma schemalist hw3 example

#+NAME: capture schemalist hw3 example
#+BEGIN_SRC sh :noweb yes :exports code :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="proforma schemalist hw3 example")>>
#+END_SRC

***** proforma schema hw3

#+NAME: capture schema_hw3
#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="proforma schema hw3")>>
#+END_SRC

***** bizfile search prive

#+NAME: capture bizfile_search_sandbox
#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="bizfile search prive")>>
#+END_SRC

***** bizfile uen jkl

JKL Technologies is a fictitious ACRA APImall Sandbox company. Queries about that UEN don't get billed.

#+NAME: capture bizfile_search_sandbox
#+BEGIN_SRC sh :noweb yes :exports results :results verbatim :tangle lgltests.sh :eval no :noweb-sep ""
<<mkCapture(myargs="bizfile uen 111111111M")>>
#+END_SRC
** Putting it all together: Infrastructure & Utilities

We set up one test script for development testing; this can be used for TDD.

We set up one test script for production testing; this is used for DevOps.

*** Resolving LGL_URI

#+NAME: range-liveness-version
#+BEGIN_SRC python :noweb yes :exports results :results output :var liveness=liveness_development :var version=version
import json
for l in liveness:
 for v in version:
  print("subdir=%s-%s LGL_URI=%s%s test_services %s" % (l[0], v[0], l[1], v[0], v[1]))
#+END_SRC

#+RESULTS: range-liveness-version
: subdir=local-v0.9 LGL_URI=http://localhost/api/v0.9 test_services --version=v0.9
: subdir=local-v1.0 LGL_URI=http://localhost/api/v1.0 test_services --version=v0.9
: subdir=local-v1.1 LGL_URI=http://localhost/api/v1.1 test_services 

#+NAME: test-prod
#+BEGIN_SRC sh  :shebang #!/bin/bash :noweb yes :tangle lgltest-prod.sh :exports results :results verbatim :eval no
<<lgltest-setup-common>>
# we test the production endpoints, across multiple versions
<<range-liveness-version(liveness=liveness_production)>>
#+END_SRC

#+NAME: test-dev
#+BEGIN_SRC sh  :shebang #!/bin/bash :noweb yes :tangle lgltest-dev.sh :exports results :results verbatim :eval no
<<lgltest-setup-common>>
# we test the development endpoints, across multiple versions
<<range-liveness-version(liveness=liveness_development)>>
#+END_SRC


#+NAME: lgl
#+BEGIN_SRC python :noweb yes :exports results :results output :var myargs="help" num=0
print('lgl %s 2>&1' % (myargs))
#+END_SRC

#+NAME: mkCapture
#+BEGIN_SRC python :noweb yes :exports results :results output :var myargs="noargs" num=0
dashed = myargs.replace(" ","-")
print('lgl $1 %s > %s.out 2> %s.err' % (myargs, dashed, dashed))
#+END_SRC

The actual test script combines all of the above.

After running the tests, we compare to see if the output of the tests match the expectations.
*** Comparing with expectations

* Testing the library

The test suite directly calls the library functions exposed by the SDK. Most of those library functions map to back-end endpoints. The output of those function calls is compared against known expectations.

* Components

** Account Creation

** Bizfile

** Proforma

** Workflow

** Payment

* Testing the endpoints

The server-side API endpoints are implicitly tested as part of the tests of the library and lgl executable.

In future, we could do curl-based testing if an independent channel of coverage is desired.

* Emacs Notes

we use org-mode babel to tangle and execute. ~C-c C-v t~ is the big one.

You want to turn off org-confirm-babel-evaluate and add Python to org-babel-load-languages.
